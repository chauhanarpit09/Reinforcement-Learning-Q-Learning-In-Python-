{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG58T9BfyKG2rWWowXGwRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chauhanarpit09/Reinforcement-Learning-Q-Learning-In-Python-/blob/master/qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ85TyrIB1le",
        "colab_type": "text"
      },
      "source": [
        "# **Q Learning FrozenLakeNoSlip-v0 Using Open Ai Gym**\n",
        "**In Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2HwdJ5wCLrW",
        "colab_type": "text"
      },
      "source": [
        "Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faymJCVdBb48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from gym.envs.registration import register\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f9Tr70UCPBa",
        "colab_type": "text"
      },
      "source": [
        "Intialize Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuyUQWfABb1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = \"FrozenLakeNoSlip-v0\"\n",
        "env = gym.make(a)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcyNAnbTCXk8",
        "colab_type": "text"
      },
      "source": [
        "Agent Which return action value when agent in exploring state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUp7amFqBbzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self,env):\n",
        "        self.action_size = env.action_space.n \n",
        "    def get_action(self,s):\n",
        "        action = random.choice(range(self.action_size))\n",
        "        return action\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bzt0a0aCjNa",
        "colab_type": "text"
      },
      "source": [
        "QAgent( Making qtable,get actions,update q value with iteration using bellmen wq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZ6Zb1ABbw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class QAgent(Agent):\n",
        "    def __init__(self,env,discount_rate = 0.97,learning_rate=0.01):\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "        self.eps = 1.0\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.build_model()\n",
        "    def build_model(self):\n",
        "        self.q_table = 1e-4*np.random.random([self.state_size,self.action_size])\n",
        "    def get_action(self,state):\n",
        "        q_state = self.q_table[state]\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = super().get_action(state)\n",
        "        return action_random if random.random() < self.eps else action_greedy\n",
        "    def train(self,exp):\n",
        "        state,action,next_state,reward,done = exp\n",
        "        q_next = self.q_table[next_state]\n",
        "        q_next = np.zeros([self.action_size]) if done else q_next\n",
        "        q_target = reward + self.discount_rate*np.max(q_next)\n",
        "        q_update = q_target - self.q_table[state,action]\n",
        "        self.q_table[state,action]+=self.learning_rate*q_update\n",
        "\n",
        "        if done:\n",
        "            self.eps = self.eps*0.99\n",
        "agent = QAgent(env)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjjhXAWICljX",
        "colab_type": "text"
      },
      "source": [
        "Wrap up ( using agent playing a FrozenLake )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSvJ84WBbvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "a1913adb-e617-4fd6-d32a-106d453c4db0"
      },
      "source": [
        "t_reward = 0\n",
        "for e in range(400):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "        agent.train((state,action,next_state,reward,done))\n",
        "        state = next_state\n",
        "        t_reward += reward\n",
        "        print(\"s:\", state, \"a:\", action,\"done:\", done)\n",
        "        print(\"Episode: {}, Total reward: {}, eps: {}\".format(e,t_reward,agent.eps))\n",
        "        env.render()\n",
        "        time.sleep(0.05)\n",
        "        clear_output(wait = True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s: 15 a: 2 done: True\n",
            "Episode: 399, Total reward: 165.0, eps: 0.017950553275045134\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I26Y2M_w_A8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}